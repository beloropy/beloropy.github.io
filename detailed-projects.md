## KRAFTON Inc. > PUBG STUDIOS (NewState) > Server Engineer

---

**DynamoDB 응답 시간 최적화**

- 기존 DynamoDB DAO에서 간헐적으로 500ms 혹은 그 이상의 응답 시간이 기록되는 것을 확인했습니다. Tracing module을 구현해 latency가 발생하는 지점이 어디인지, TLS Handshake가 어느 정도 시간이 소요되는지 등을 파악하며 응답 지연의 원인을 분석했습니다. DynamoDB Client는 Credentials Token을 caching하고 있지만 해당 token이 만료되었을 경우 이를 재발급한 뒤 operation을 수행하는 것을 확인했습니다.

- AWS credential token을 background에서 갱신하는 모듈을 구현해 응답 지연을 빈도를 줄였습니다. 또한 AWS DynamoDB의 설계를 고려해 오래 걸리는 요청을 기다리는 대신, 짧은 타임아웃으로 재처리를 시도하는 정책을 갖도록 클라이언트를 개선했습니다.

---

**인앱 일회성 상품 결제 프로세스 개선**

GooglePlay 인앱 일회성 상품 결제 프로세스에서 클라이언트 대신 서버가 consume 처리를 하도록 개선해 abusing을 방지했습니다.

---

**인앱 정기 결제 시스템 개발**

- GooglePlay, AppStore 인앱 정기 결제 기반의, 정기 결제 상태 관리 및 보상 지급 시스템을 설계 및 구현했습니다. 비즈니스적 유연성과 확장성을 염두에 둔 설계 덕분에 초기 사양의 변경 사항에 대해 최소한의 코드 수정으로 대응할 수 있었습니다.

- GooglePlay로부터 사용자의 정기 결제 상태 변경에 대한 notification을 받을 수 있도록 GCP Pub/Sub을 구축했습니다. 인바운드 트래픽을 처리할 수 있도록 DevOps 팀과 협력해 Ingress를 설정하고 핸들러를 구현했습니다.

---

**컨텐츠 상점 시스템 구현**

클랜, 명예 등 다른 시스템의 서브 컨텐츠로 사용되는 각 상점 기능들을 컨텐츠 상점 시스템으로 통합 관리하고 있습니다.

---

**클랜 관련 신규 컨텐츠 구현**

클랜 경험치, 시즌별 활동지수, 마일리지, 랭킹, 미션 및 보상 수령 기능 등 전반을 구현했습니다. 가중치 기반 무작위 추출 모듈을 개선하고 유닛 테스트를 추가했습니다.

---

**랭킹, 커스텀 매치 서비스 최적화**

- 랭킹 서비스가 bulk operation을 처리할 수 있도록 개선해 network I/O를 줄이고, 동기적으로 처리되었던 랭킹 관련 업데이트를 비동기적으로 처리해 response time을 감소시켰습니다.

- 커스텀 매치 시스템에 redis hashes를 적용하고 application-level 캐시를 구현해 성능을 최적화했습니다.

---

**시즌, 티어, 대전기록, 전적 기능 확장 및 개선**

시즌, 티어, 대전기록, 전적 서비스에 대해서 기능 확장을 담당하고 있습니다. 신규 게임 타입 및 BountyRoyale 등의 게임 모드 추가로 인한 요구 사항에 대응했습니다. 시즌 관련 모듈을 리팩토링하고 다른 서비스에 갖고 있는 의존성을 제거했습니다.

## IGAWorks > DFINERY (Growth Action) > Back-end Engineer

---

**개인화 메시지 개발**

- 개인화 메시지 프로젝트의 리드를 맡았습니다. 개인화 메시지 기능의 PoC를 진행하고 다른 서비스를 레퍼런스로 삼아, 초기 단계의 product를 기획했습니다.

- 메시지 개인화 모듈을 구현했습니다. 메시지를 개인화하냐 여부에 따라 각 채널별 메시징 API spec에 최적화된 payload를 구성할 수 있도록 메시지 전송 데이터 구조 및 파이프라인을 개선했습니다.

- 모듈 구현 후 성능 테스트를 거친 뒤 캐싱, 분산 처리, 내부 템플릿 최적화 등을 통해 메시지 전송 파이프라인에서의 실행 시간을 55% 이상 단축시켰습니다.

<!--
개인화 메시지 기능을 개발한 후 성능 테스트를 했을 때, 개인화된 푸시 메시지를 보내는 데에 걸리는 시간이 기존(개인화되지 않은 푸시 메시지)에 비해 2배 이상 늘어났습니다.

개인화를 위한 사용자 정보 접근, 파라미터 셋 구성, 프로퍼티 치환 등 각 파이프라인에 로그를 심고 확인해본 결과 사용자 정보를 파라미터로 사용해 메시지 프로퍼티를 치환하는 데에 걸리는 오버헤드도 분명 있지만, 그 외의 전처리 작업 등에서 훨씬 더 큰 시간이 소요되고 있다는 것을 확인했습니다.

메시지 프로퍼티의 치환 자체는 다른 라이브러리의 기능을 사용하고 있었고, 이 워크로드를 최적화하기 어려웠습니다. 때문에 치환을 제외한 나머지 파이프라인에서의 최적화를 고민해야 했습니다.

우선, 메시지 프로퍼티를 치환하기 위해서는 전처리된 템플릿 인스턴스가 필요했는데 이들을 인메모리 캐싱 후 재사용하는 전략을 사용했습니다. 본래 해당 라이브러리에서는 개인화마다 개별 템플릿 인스턴스 생성 후 이를 사용하기를 권장했고 그렇게 모듈을 설계했습니다. 하지만 소스 코드 분석 이후 여러 thread에서 동시에 해당 인스턴스에 접근하지 않는다면 문제가 되지 않는다는 판단을 내렸고 이를 지키며 로직을 구성했습니다.

또한 메시지 포맷을 분석해 사용자의 정보와 특정 표현식이 치환 함수에 입력되더라도 특정 프로퍼티가 바뀌지 않을 경우, 혹은 바뀔 값을 미리 알 수 있는 경우를 찾는 전처리 모듈을 개발해 이러한 케이스들을 스킵하도록 했습니다.

마지막으로 DB를 조회해 사용자 정보를 얻는 과정을 분산처리하고 사용자 정보 중 전체 메시지 포맷에서 사용되지 않는 정보들을 미리 제거해, 다루는 데이터의 양을 줄였습니다.

이러한 최적화를 적용하기 전, 전송 모듈 성능 테스트 환경에서 메시지 당 1건의 개인화가 필요한 푸시 메시지 100만 건을 처리하는 데에 걸리는 시간이 5분 이상이었는데 최적화 후 실행 시간을 2분 중반, 약 55% 수준으로 감소시켰습니다. 이는 개인화를 적용하지 않는 것과 비교했을 때 비슷한 수준의 실행 시간이었습니다.
-->

---

**APNS Token 관리 개선**

메시지 전송 모듈이 APNS API 호출을 위한 토큰을 해당 관리 모듈로부터 획득하고 이를 갱신 요청하는 프로세스를 개선했습니다. 모듈 단위의 유닛 테스트 및 통합 테스트를 설계해 전체 시스템의 안전성을 높였습니다.

---

**메시지 채널 확장**

- 카카오톡, SMS 채널에 대한 메시지 전송 모듈을 구현했습니다.

- 여러 전송 채널을 고려한 대체 발송 기능을 포함해 메시지 전송 파이프라인을 개선 및 리팩토링했습니다. 앞서 구현한 전송 모듈과 연결해 카카오톡, SMS 메시징 기능을 개발했습니다.

---

**메시징 API 개발 및 코어 모듈 개선**

- Console에서 사용되는 test push notification 등의 신규 메시징 API를 개발했습니다. Serverless architecture의 이점이 기대되어 AWS Lambda에 해당 application을 배포했으며, 기존 jenkins CI/CD 파이프라인을 기반으로 Lambda 배포를 자동화했습니다.

- push message 스케줄링 모듈을 SRP 원칙에 따라 리팩토링해 4개의 submodule로 분리하고 각 module에 유닛 테스트를 새로 추가했습니다. 통합 테스트를 설계해 기존 모듈 수준에서의 시나리오를 검증했습니다.